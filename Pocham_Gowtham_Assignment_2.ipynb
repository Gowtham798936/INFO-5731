{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "outputId": "a2244528-24bb-4282-8a39-d4c909224fe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews saved to movie_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_imdb_reviews(movie_id, max_reviews=1000):\n",
        "    url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        reviews = soup.find_all('div', class_='text show-more__control')\n",
        "        reviews_data = []\n",
        "        for review in reviews[:max_reviews]:\n",
        "            text = review.get_text(strip=True)\n",
        "            reviews_data.append(text)\n",
        "        return reviews_data\n",
        "    else:\n",
        "        print(\"Failed to fetch reviews\")\n",
        "        return []\n",
        "#(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Review'])\n",
        "        writer.writerows([[review] for review in data])\n",
        "\n",
        "def main():\n",
        "    # Example movie ids\n",
        "    movie_ids = [\"tt0172495\", \"tt0111161\"]  # The Matrix, The Shawshank Redemption\n",
        "\n",
        "    all_reviews = []\n",
        "    for movie_id in movie_ids:\n",
        "        reviews = scrape_imdb_reviews(movie_id)\n",
        "        all_reviews.extend(reviews)\n",
        "\n",
        "    if all_reviews:\n",
        "        save_to_csv(all_reviews, 'movie_reviews.csv')\n",
        "        print(\"Reviews saved to movie_reviews.csv\")\n",
        "    else:\n",
        "        print(\"No reviews found\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044418fd-7a44-4e89-9c59-23c3560a5a0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to cleaned_movie_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "#(1) Remove noise, such as special characters and punctuations.\n",
        "def remove_special_characters(text):\n",
        "    # Remove special characters and punctuation\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "#(2) Remove numbers.\n",
        "def remove_numbers(text):\n",
        "    # Remove numbers\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "#(3) Remove stopwords by using the stopwords list.\n",
        "def remove_stopwords(text):\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "#(5) Stemming.\n",
        "def stemming(text):\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stemmed_text = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stemmed_text)\n",
        "#(6) Lemmatization.\n",
        "def lemmatization(text):\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(lemmatized_text)\n",
        "#(4) Lowercase all texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = stemming(text)\n",
        "    text = lemmatization(text)\n",
        "    return text\n",
        "\n",
        "def clean_and_save_csv(input_file, output_file):\n",
        "    with open(input_file, 'r', newline='', encoding='utf-8') as infile, \\\n",
        "            open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        fieldnames = reader.fieldnames + ['Cleaned_Review']\n",
        "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in reader:\n",
        "            cleaned_review = clean_text(row['Review'])\n",
        "            row['Cleaned_Review'] = cleaned_review\n",
        "            writer.writerow(row)\n",
        "\n",
        "def main():\n",
        "    input_file = 'movie_reviews.csv'\n",
        "    output_file = 'cleaned_movie_reviews.csv'\n",
        "    clean_and_save_csv(input_file, output_file)\n",
        "    print(\"Cleaned data saved to cleaned_movie_reviews.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb372e0-b91d-4a75-817b-ba5f06e84de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Parts of Speech (POS) Tagging:\n",
            "Counter({'PROPN': 5, 'ADJ': 4, 'DET': 3, 'NOUN': 3, 'PUNCT': 3, 'VERB': 2, 'ADP': 2, 'AUX': 1})\n",
            "\n",
            "Constituency Parsing Trees:\n",
            "\n",
            "Dependency Parsing Trees:\n",
            "The <--det-- fox\n",
            "quick <--amod-- fox\n",
            "brown <--amod-- fox\n",
            "fox <--nsubj-- jumps\n",
            "jumps <--ROOT-- jumps\n",
            "over <--prep-- jumps\n",
            "the <--det-- dog\n",
            "lazy <--amod-- dog\n",
            "dog <--pobj-- over\n",
            ". <--punct-- jumps\n",
            "John <--nsubj-- works\n",
            "works <--ROOT-- works\n",
            "at <--prep-- works\n",
            "Google <--pobj-- at\n",
            ". <--punct-- works\n",
            "New <--compound-- York\n",
            "York <--compound-- City\n",
            "City <--nsubj-- is\n",
            "is <--ROOT-- is\n",
            "a <--det-- metropolis\n",
            "bustling <--amod-- metropolis\n",
            "metropolis <--attr-- is\n",
            ". <--punct-- is\n",
            "\n",
            "Named Entity Recognition (NER):\n",
            "Counter({'PERSON': 1, 'ORG': 1, 'GPE': 1})\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from nltk import CFG\n",
        "from nltk.parse import ChartParser\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Updated grammar\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> Det N | N\n",
        "VP -> V NP | V PP\n",
        "PP -> P NP\n",
        "Det -> 'the' | 'a' | 'an' | 'The' | '.'\n",
        "N -> 'dog' | 'cat' | 'bird' | 'man' | 'woman' | 'fox' | 'John' | 'New' | 'York' | 'City'| 'at' | 'Google'| 'is' | 'bustling' | 'metropolis'\n",
        "V -> 'jumps' | 'runs' | 'flies' | 'walks' | 'over' | 'works'\n",
        "P -> 'over'\n",
        "ADJ -> 'quick' | 'brown' | 'lazy'\n",
        "\"\"\")\n",
        "# Rerun the code\n",
        "def pos_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = Counter(token.pos_ for token in doc)\n",
        "    return pos_counts\n",
        "\n",
        "def constituency_parsing(text):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sent in sentences:\n",
        "        # Tokenize the sentence into words\n",
        "        words = nltk.word_tokenize(sent)\n",
        "        # Perform constituency parsing\n",
        "        parser =nltk.ChartParser(grammar)\n",
        "        for tree in parser.parse(words):\n",
        "            print(tree)\n",
        "\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    print(\"\\nDependency Parsing Trees:\")\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            print(f\"{token.text} <--{token.dep_}-- {token.head.text}\")\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    entities = Counter(ent.label_ for ent in doc.ents)\n",
        "    return entities\n",
        "\n",
        "def main():\n",
        "    # Example cleaned text\n",
        "    cleaned_text = \"The quick brown fox jumps over the lazy dog. John works at Google. New York City is a bustling metropolis.\"\n",
        "\n",
        "    # Parts of Speech (POS) Tagging\n",
        "    pos_counts = pos_tagging(cleaned_text)\n",
        "    print(\"Parts of Speech (POS) Tagging:\")\n",
        "    print(pos_counts)\n",
        "\n",
        "    # Constituency Parsing\n",
        "    print(\"\\nConstituency Parsing Trees:\")\n",
        "    constituency_parsing(cleaned_text)\n",
        "\n",
        "    # Dependency Parsing\n",
        "    dependency_parsing(cleaned_text)\n",
        "\n",
        "    # Named Entity Recognition (NER)\n",
        "    entities = named_entity_recognition(cleaned_text)\n",
        "    print(\"\\nNamed Entity Recognition (NER):\")\n",
        "    print(entities)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The above code was most challeging to execute and learned a lot."
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}